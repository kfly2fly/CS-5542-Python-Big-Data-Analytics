{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S7A5Yj4E9Gz"
      },
      "source": [
        "Import our dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ytaPiTOaAz7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hLk4Dz-aTKa"
      },
      "source": [
        "Download the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx7EhUpcaVES"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file('gatsby.txt', 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WApqqpagatXt"
      },
      "source": [
        "**Read the data. In this ICP we are using the book 'The Great Gatsby' as our dataset.**\n",
        "\n",
        "Our data had some legal information attached to it, so we decided to slice that part off. \n",
        "\n",
        "First, explore the text by looking at the length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvqlq8bwaam5",
        "outputId": "e469a015-dd0c-4270-af30-8484b2ab0ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 277003 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for compatability\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Slice text to get rid of all legal disclosures, we only want the raw text from the book\n",
        "text = text[908:-18762]\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BYk1xhLa8V4",
        "outputId": "6bc13ca6-de34-43fd-a9d8-3873494d5310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\t   The Great Gatsby\r\n",
            "\t\t\t\t  by\r\n",
            "\t\t\t F. Scott Fitzgerald\r\n",
            "\r\n",
            "\r\n",
            "                           Table of Contents\r\n",
            "\r\n",
            "I\r\n",
            "II\r\n",
            "III\r\n",
            "IV\r\n",
            "V\r\n",
            "VI\r\n",
            "VII\r\n",
            "VIII\r\n",
            "IX\r\n",
            "\r\n",
            "\r\n",
            "                              Once again\r\n",
            "                                  to\r\n",
            "                                 Zelda\r\n",
            "\r\n",
            "  Then wear the gold hat, if that will move her;\r\n",
            "  If you can bounce high, bounce for her too,\r\n",
            "  Till she cry “Lover, gold-hatted, high-bouncing lover,\r\n",
            "  I must have you!”\r\n",
            "\r\n",
            "  Thomas Parke d’Invilliers\r\n",
            "\r\n",
            "\r\n",
            "                                  I\r\n",
            "\r\n",
            "In my younger and more vulnerable years my father gave me some advice\r\n",
            "that I’ve been turning over in my mind ever since.\r\n",
            "\r\n",
            "“Whenever you feel like criticizing anyone,” he told me, “just\r\n",
            "remember that all the people in this world haven’t had the advantages\r\n",
            "that you’ve had.”\r\n",
            "\r\n",
            "He didn’t say any more, but we’ve always been unusually communicative\r\n",
            "in a reserved way, and I understood that he meant a great deal more\r\n",
            "than that. In consequence, I’m inclined to reserve all\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 1000 characters in text\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIbQmwUkrFfY"
      },
      "source": [
        "**Data Exploration**\n",
        "\n",
        "Lets group the text by character to see the frequency distribution of each character"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5ZTez6YF3q7"
      },
      "source": [
        "This is the frequency distribution for the raw data. We use the Counter module to aggregate the characters into buckets. We can then make decisions about preprocessing the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svhdw7btkE4_",
        "outputId": "7a8447e5-62f4-47c2-ac6d-55f4a8f03df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of all characters in Gatsby is :\n",
            " Counter({' ': 44191, 'e': 25006, 't': 18096, 'a': 16841, 'o': 15738, 'n': 14063, 'i': 12528, 's': 12367, 'h': 12237, 'r': 11339, 'd': 9609, 'l': 8173, '\\r': 6399, '\\n': 6398, 'u': 5833, 'm': 5205, 'w': 4950, 'g': 4520, 'c': 4387, 'y': 4367, 'f': 4110, '.': 3107, 'p': 2993, 'b': 2975, ',': 2967, '-': 2047, 'k': 1927, 'v': 1883, 'I': 1671, '“': 1457, '”': 1455, '’': 1346, 'T': 690, 'W': 498, 'H': 421, '—': 417, 'G': 376, '?': 328, 'S': 324, 'A': 320, 'M': 314, 'D': 307, 'x': 288, 'B': 223, 'j': 203, 'Y': 181, 'C': 171, 'N': 163, 'q': 156, 'z': 145, 'O': 142, 'J': 142, '!': 124, 'E': 122, 'F': 97, 'L': 96, ':': 81, ';': 74, 'P': 65, '…': 54, 'R': 40, 'K': 37, 'V': 29, '‘': 25, '0': 22, 'é': 17, '1': 16, '\\u200a': 12, '9': 11, '\\t': 10, '5': 9, 'U': 9, '(': 7, ')': 7, '3': 7, '*': 6, '6': 5, 'Q': 4, '2': 4, '8': 3, 'X': 2, '7': 2, '4': 2, '[': 2, ']': 2, '$': 2, 'Z': 1, 'ô': 1, 'ê': 1, 'ç': 1})\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter                                                \n",
        "  \n",
        "# using collections.Counter() to get \n",
        "# count of each element in string \n",
        "res = Counter(text)\n",
        "  \n",
        "# printing result \n",
        "print (\"Count of all characters in Gatsby is :\\n \" +  str(res))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnTGdNJcrMbE"
      },
      "source": [
        "We have decided to remove some characters and replace others. \n",
        "\n",
        "This reduction of data will hopefully help the model to learn faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tcl5gGLYm_n8",
        "outputId": "3a144cf8-0fe9-414a-b32a-a7bc19305b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of filtered characters in Gatsby is :\n",
            " Counter({' ': 44203, 'e': 25024, 't': 18096, 'a': 16841, 'o': 15739, 'n': 14063, '\\n': 12797, 'i': 12528, 's': 12367, 'h': 12237, 'r': 11339, 'd': 9609, 'l': 8173, 'u': 5833, 'm': 5205, 'w': 4950, 'g': 4520, 'c': 4388, 'y': 4367, 'f': 4110, '.': 3107, 'p': 2993, 'b': 2975, ',': 2967, '-': 2047, 'k': 1927, 'v': 1883, 'I': 1671, '“': 1457, '”': 1455, '’': 1346, 'T': 690, 'W': 498, 'H': 421, '—': 417, 'G': 376, '?': 328, 'S': 324, 'A': 320, 'M': 314, 'D': 307, 'x': 288, 'B': 223, 'j': 203, 'Y': 181, 'C': 171, 'N': 163, 'q': 156, 'z': 145, 'O': 142, 'J': 142, '!': 124, 'E': 122, 'F': 97, 'L': 96, ':': 81, ';': 74, 'P': 65, '…': 54, 'R': 40, 'K': 37, 'V': 29, '‘': 25, '0': 22, '1': 16, '9': 11, '\\t': 10, '5': 9, '(': 9, ')': 9, 'U': 9, '3': 7, '*': 6, '6': 5, 'Q': 4, '2': 4, '8': 3, 'X': 2, '7': 2, '4': 2, 'Z': 1})\n"
          ]
        }
      ],
      "source": [
        "# Use replace() to remove characters and replace with another\n",
        "filtered_text = text.replace('\\r', '\\n').replace('\\u200a', ' ').replace('é', 'e').replace('[', '(').replace(']', ')').replace('$', '').replace('ô','o').replace('ê','e').replace('ç','c')\n",
        "\n",
        "# using collections.Counter() to get \n",
        "# count of each element in string \n",
        "char_freq = Counter(filtered_text)\n",
        "  \n",
        "# printing result \n",
        "# we now have removed some of the outliers\n",
        "print (\"Count of filtered characters in Gatsby is :\\n \" +  str(char_freq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axr1Lof9ri0O"
      },
      "source": [
        "By replacing characters in the original text, we have reduced the number of unique characters from 90 to 81."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkJ_5C5ybOq_",
        "outputId": "3577153d-35e2-4033-c2dd-7b071d8ef06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 unique characters\n",
            "['Z', 'X', '7', '4', '8', 'Q', '2', '6', '*', '3', '5', '(', ')', 'U', '\\t', '9', '1', '0', '‘', 'V', 'K', 'R', '…', 'P', ';', ':', 'L', 'F', 'E', '!', 'O', 'J', 'z', 'q', 'N', 'C', 'Y', 'j', 'B', 'x', 'D', 'M', 'A', 'S', '?', 'G', '—', 'H', 'W', 'T', '’', '”', '“', 'I', 'v', 'k', '-', ',', 'b', 'p', '.', 'f', 'y', 'c', 'g', 'w', 'm', 'u', 'l', 'd', 'r', 'h', 's', 'i', '\\n', 'n', 'o', 'a', 't', 'e', ' ']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "\n",
        "#Instead of having the vocab letters in an arbitrary format, we can sort the letters based on frequency\n",
        "#For example, 'Z' has the lowest frequency of all letters and will get assigned the index of 0\n",
        "\n",
        "#Hopefully this will have an impact on the learning\n",
        "vocab = sorted(char_freq, key=char_freq.get, reverse=False)\n",
        "print ('{} unique characters'.format(len(vocab)))\n",
        "#Heres the list of all characters that appear in the text\n",
        "print(vocab)                                                                  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6LKRZvbYx1"
      },
      "source": [
        "## Process the text\n",
        "\n",
        "Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlJwkz5RbdMw",
        "outputId": "d28828d7-aded-4c12-9ba3-9e2c5ef5b7d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 80 53 74 74 74\n",
            " 74 53 75 80 66 62 80 62 76 67 75 64 79 70 80 77 75 69 80 66 76 70 79 80\n",
            " 54 67 68 75 79 70 77 58 68 79 80 62 79 77 70 72 80 66 62 80 61 77 78 71\n",
            " 79 70 80 64 77 54 79 80 66 79 80 72 76 66 79 80 77 69 54 73 63 79 74 74\n",
            " 78 71 77 78]\n",
            "'                    I\\n\\n\\n\\nIn my younger and more vulnerable years my father gave me some advice\\n\\nthat'\n"
          ]
        }
      ],
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "#Create a dictionary, i tracks the index of the char, u tracks the char\n",
        "char2idx = dict((u,i) for i, u in enumerate(vocab))\n",
        "\n",
        "#can access a char based on index\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in filtered_text])\n",
        "#the corpus has been turned into an index for each char\n",
        "print(text_as_int[500:600])\n",
        "\n",
        "print(repr(''.join(idx2char[text_as_int[500:600]])))                                     #edit me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TJxiSQ1hhNW"
      },
      "source": [
        "The prediction task\n",
        "\n",
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform. The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n",
        "\n",
        "Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of seq_length+1. For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the tf.data.Dataset.from_tensor_slices function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtbQZTCdI-nX"
      },
      "source": [
        "**We have changed the sequence length**\n",
        "\n",
        "We hope that by changing the sequence length to be longer, the LSTM will have more information to learn from.\n",
        "\n",
        "Doing this increases the number of time steps in which the model has to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuF_yJGthnvN",
        "outputId": "7df3c989-c137-4446-94a4-49c4713b2da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>\n",
            "\t\n",
            "tf.Tensor(14, shape=(), dtype=int64)\n",
            "\t\n",
            "tf.Tensor(14, shape=(), dtype=int64)\n",
            "\t\n",
            "tf.Tensor(14, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(80, shape=(), dtype=int64)\n",
            " \n",
            "tf.Tensor(80, shape=(), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 200                                                               \n",
        "\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "# we use tensorflow dataset because it is good for streaming data\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "print(type(char_dataset))\n",
        "\n",
        "#take() displays first x members of the tensor DataSet. For each element, we can get the data by calling .numpy() method\n",
        "# we can then enter i into our lookup array to get the resultant character\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtyvL4vFiDW9"
      },
      "source": [
        "The batch method lets us easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDzAi9mmiG6J",
        "outputId": "d5db224d-78da-4ac8-fab3-573045b04e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'\\t\\t\\t   The Great Gatsby\\n\\n\\t\\t\\t\\t  by\\n\\n\\t\\t\\t F. Scott Fitzgerald\\n\\n\\n\\n\\n\\n                           Table of Contents\\n\\n\\n\\nI\\n\\nII\\n\\nIII\\n\\nIV\\n\\nV\\n\\nVI\\n\\nVII\\n\\nVIII\\n\\nIX\\n\\n\\n\\n\\n\\n                              Once again\\n\\n      '\n",
            "'                            to\\n\\n                                 Zelda\\n\\n\\n\\n  Then wear the gold hat, if that will move her;\\n\\n  If you can bounce high, bounce for her too,\\n\\n  Till she cry “Lover, gold-ha'\n",
            "'tted, high-bouncing lover,\\n\\n  I must have you!”\\n\\n\\n\\n  Thomas Parke d’Invilliers\\n\\n\\n\\n\\n\\n                                  I\\n\\n\\n\\nIn my younger and more vulnerable years my father gave me some advice\\n\\nthat I’'\n",
            "'ve been turning over in my mind ever since.\\n\\n\\n\\n“Whenever you feel like criticizing anyone,” he told me, “just\\n\\nremember that all the people in this world haven’t had the advantages\\n\\nthat you’ve had.”\\n\\n'\n",
            "'\\n\\nHe didn’t say any more, but we’ve always been unusually communicative\\n\\nin a reserved way, and I understood that he meant a great deal more\\n\\nthan that. In consequence, I’m inclined to reserve all judg'\n"
          ]
        }
      ],
      "source": [
        "#group the digits into strings of length 101\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "#Heres an example of changing a sequence of numbers back into alphanumeric chars\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lpa_PJ1kilMf"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Fz9l6xTimnV",
        "outputId": "d23e94e6-6e8a-41c1-db6c-e7ac0a87261f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
            "array([14, 14, 14, 80, 80, 80, 49, 71, 79, 80, 45, 70, 79, 77, 78, 80, 45,\n",
            "       77, 78, 72, 58, 62, 74, 74, 14, 14, 14, 14, 80, 80, 58, 62, 74, 74,\n",
            "       14, 14, 14, 80, 27, 60, 80, 43, 63, 76, 78, 78, 80, 27, 73, 78, 32,\n",
            "       64, 79, 70, 77, 68, 69, 74, 74, 74, 74, 74, 74, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 80, 49, 77, 58, 68, 79, 80, 76, 61, 80, 35, 76, 75,\n",
            "       78, 79, 75, 78, 72, 74, 74, 74, 74, 53, 74, 74, 53, 53, 74, 74, 53,\n",
            "       53, 53, 74, 74, 53, 19, 74, 74, 19, 74, 74, 19, 53, 74, 74, 19, 53,\n",
            "       53, 74, 74, 19, 53, 53, 53, 74, 74, 53,  1, 74, 74, 74, 74, 74, 74,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 30, 75, 63, 79,\n",
            "       80, 77, 64, 77, 73, 75, 74, 74, 80, 80, 80, 80, 80])>, <tf.Tensor: shape=(200,), dtype=int64, numpy=\n",
            "array([14, 14, 80, 80, 80, 49, 71, 79, 80, 45, 70, 79, 77, 78, 80, 45, 77,\n",
            "       78, 72, 58, 62, 74, 74, 14, 14, 14, 14, 80, 80, 58, 62, 74, 74, 14,\n",
            "       14, 14, 80, 27, 60, 80, 43, 63, 76, 78, 78, 80, 27, 73, 78, 32, 64,\n",
            "       79, 70, 77, 68, 69, 74, 74, 74, 74, 74, 74, 80, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 49, 77, 58, 68, 79, 80, 76, 61, 80, 35, 76, 75, 78,\n",
            "       79, 75, 78, 72, 74, 74, 74, 74, 53, 74, 74, 53, 53, 74, 74, 53, 53,\n",
            "       53, 74, 74, 53, 19, 74, 74, 19, 74, 74, 19, 53, 74, 74, 19, 53, 53,\n",
            "       74, 74, 19, 53, 53, 53, 74, 74, 53,  1, 74, 74, 74, 74, 74, 74, 80,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
            "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 30, 75, 63, 79, 80,\n",
            "       77, 64, 77, 73, 75, 74, 74, 80, 80, 80, 80, 80, 80])>)\n"
          ]
        }
      ],
      "source": [
        "#Create a function to split the data\n",
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "#Create a mappedDataset object. For each item in this dataset, there is the input or training array and the output or testing array\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#We can see that the resultant is 2 numpy arrays for each batch\n",
        "# The first array is the input and the second is the target\n",
        "for i in dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0TUAUR_i39T"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTXfK1w6jtTg"
      },
      "source": [
        "Create training batches\n",
        "\n",
        "We used tf.data to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAScADqMPBHs"
      },
      "source": [
        "**We added the .repeat() helper to the end of the shuffle method**\n",
        "\n",
        "This will allow the model to repeatable and so it will never end. This gives us a huge advantage as Deep Learning needs to have lots of data. This allows us to increase our Steps per epoch in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hpe5_lsee4dg",
        "outputId": "063fd7b5-4120-4b62-e76b-1d4ab87d8dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset element_spec=(TensorSpec(shape=(64, 200), dtype=tf.int64, name=None), TensorSpec(shape=(64, 200), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# Change the Batch size to 128\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "#Add the repeat() helper so that the dataset is endless\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYEKPJ-NkbSh"
      },
      "source": [
        "Build The Model\n",
        "\n",
        "Use tf.keras.Sequential to define the model. For this simple example three layers are used to define our model:\n",
        "\n",
        "tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
        "\n",
        "tf.keras.layers.GRU: A type of RNN with size units=rnn_units (You can also use a LSTM layer here.)\n",
        "\n",
        "tf.keras.layers.Dense: The output layer, with vocab_size outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHaiwI1_knUL"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024                                                                                                          #KF: we could increase this "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyaPQ0kRvWUn"
      },
      "source": [
        "**Adding a LSTM layer**\n",
        "LSTM has a memory cell which can hold information in memory for a longer period of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. We could have used the GRU but the problem with GRU is that it doesnt have the seperate memory cell and they have fewer gate cells.\n",
        "\n",
        "\n",
        "\n",
        "**Adding a Dropuout layer**\n",
        "We have added a Dropout Layer of Dropout(0.1) which means that 10% of data will be dropped out which aims to decrease the prossibility of overfitting ,thus we can decrease the loss and make a better prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqDUtzUblPuG"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    #The embedding layer helps to add semantic meaning to the input\n",
        "    #With embedding we can do word math such as   King - man + woman = Queen\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    # The LSTM layer brings in the concept of cell state. These cell states are \n",
        "    # how the network remembers what has previously been entered\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    # The dropout layer prevents overfitting\n",
        "    tf.keras.layers.Dropout(0.1) ,  \n",
        "    # The dense layer is where the model decides what to output                 \n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "  ])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J9HRbAqRTk5"
      },
      "source": [
        "Here we build the model with the chosen params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ4QMEsMlZV6"
      },
      "outputs": [],
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzqB_vXQlsra"
      },
      "source": [
        "Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGTwvn-9mch1",
        "outputId": "a718c798-2083-4790-d609-10ee5d0f84b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "(64, 200, 81) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw4GhU0ImwZ_"
      },
      "source": [
        "In the above example, the sequence length of the input is **XXXXXX** but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxKU4npylbZs",
        "outputId": "89884f5c-1585-4df1-8c16-98862fc101b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           20736     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dropout (Dropout)           (64, None, 1024)          0         \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 81)            83025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,350,737\n",
            "Trainable params: 5,350,737\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# We can summarize the model to get information about the layers\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY9RA1IjnVPy"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to sample from this distribution as taking the argmax of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klpi-b6ZnA4v",
        "outputId": "bf619fe0-67d7-4ead-f566-3138e6909149"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([47, 21,  5, 39, 42, 61, 27, 56, 22, 55, 35, 45, 22, 67, 30, 37, 49,\n",
              "       19, 22, 32, 28, 71, 17, 39, 37, 27, 57, 26, 29, 60,  0,  1,  4, 76,\n",
              "       15, 19, 49, 48, 45, 44, 46,  0, 37, 65, 42, 42, 32, 16, 73, 24, 58,\n",
              "       74, 53, 31, 40, 56, 40, 34, 73, 21, 17, 49, 21, 78, 40, 50, 25, 42,\n",
              "       47,  4, 60, 54, 76, 14, 73, 41, 40,  8, 18, 50, 73, 75, 24,  3, 47,\n",
              "       70,  4,  1, 40,  1, 52, 19, 16, 31, 13, 22, 50, 45, 66, 47, 79,  2,\n",
              "       28, 27,  8, 38, 75,  9, 22, 60, 22, 22, 69, 80, 32, 12, 37, 28, 44,\n",
              "       56, 31,  4, 74, 73, 18, 44, 58, 78, 38, 55, 71, 37, 58,  9, 44, 42,\n",
              "       16, 47,  7, 54, 48, 50, 52, 78, 69, 49, 18, 75, 22, 36, 79, 14, 68,\n",
              "       42, 39, 21, 64, 39, 25,  4, 68, 19,  4,  5, 14, 53, 73, 54,  8, 45,\n",
              "       53, 40, 11, 75, 62, 21, 61, 52, 60, 37, 45, 62, 37, 32,  2, 28,  1,\n",
              "       45, 80, 19, 38, 59, 13, 30, 32, 68, 52, 74, 63, 29])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# We have not actually run the model, so this information will be gibberish\n",
        "# However it helps us to understand what is going on in the model\n",
        "# We use random so that the model does not output the same thing for similar inputs\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "#This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cpt66icn3LZ"
      },
      "source": [
        "Train the model\n",
        "\n",
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n",
        "\n",
        "Attach an optimizer, and a loss function\n",
        "The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because our model returns logits, we need to set the from_logits flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL77odsVn7G4",
        "outputId": "7edb659a-3dca-4b2a-9bae-645d127690a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 200, 81)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.3935976\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function, we use sparse because our data is not one hot encoded\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Before we run the model, lets see what the loss output will look like\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAZQZedVoRzi"
      },
      "source": [
        "Configure the training procedure using the tf.keras.Model.compile method. We'll use tf.keras.optimizers.Adam with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROcK85vxoSGB"
      },
      "outputs": [],
      "source": [
        "# Compile the model with the adam optimizer\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wsLm3DcolOB"
      },
      "source": [
        "Configure checkpoints\n",
        "\n",
        "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBjDBQ6iooRs"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rjdFE8gVU72"
      },
      "source": [
        "We also define an early stopping callback. This callback will allow the model to stop if the loss does not keep improving. \n",
        "\n",
        "With this callback, we can use large number of epochs but the model will stop after the optimal loss is found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGcQi9gafnjc"
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    monitor='loss',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqctwrjRoyol"
      },
      "source": [
        "Execute the training\n",
        "\n",
        "To keep training time reasonable, use 10 epochs to train the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2RhNKdeuR03"
      },
      "source": [
        "### **Changing Epoch size from 10 to 50.**\n",
        "\n",
        "Increasing the epoch size aims to decrease the loss. The increased epoch size is to provide more iterations with our dataset. \n",
        "\n",
        "Increased epochs allow for more learning to be done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmWr6teAfYWw"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "INITIAL_EPOCH = 1\n",
        "STEPS_PER_EPOCH = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4Xhz4YXo_WF",
        "outputId": "946b22a0-faf3-456d-e6d7-d5440573ba57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "200/200 [==============================] - 23s 109ms/step - loss: 2.4904\n",
            "Epoch 2/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.7648\n",
            "Epoch 3/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 1.4457\n",
            "Epoch 4/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.1924\n",
            "Epoch 5/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.9054\n",
            "Epoch 6/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.6007\n",
            "Epoch 7/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.3910\n",
            "Epoch 8/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.2830\n",
            "Epoch 9/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.2263\n",
            "Epoch 10/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1971\n",
            "Epoch 11/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1766\n",
            "Epoch 12/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1622\n",
            "Epoch 13/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1517\n",
            "Epoch 14/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1436\n",
            "Epoch 15/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1368\n",
            "Epoch 16/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1313\n",
            "Epoch 17/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1256\n",
            "Epoch 18/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1208\n",
            "Epoch 19/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.1177\n",
            "Epoch 20/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1144\n",
            "Epoch 21/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1125\n",
            "Epoch 22/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.1077\n",
            "Epoch 23/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1055\n",
            "Epoch 24/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1034\n",
            "Epoch 25/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.1006\n",
            "Epoch 26/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0987\n",
            "Epoch 27/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.0991\n",
            "Epoch 28/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0951\n",
            "Epoch 29/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0926\n",
            "Epoch 30/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0914\n",
            "Epoch 31/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0975\n",
            "Epoch 32/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0976\n",
            "Epoch 33/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0839\n",
            "Epoch 34/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0790\n",
            "Epoch 35/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0817\n",
            "Epoch 36/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1850\n",
            "Epoch 37/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.1110\n",
            "Epoch 38/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0788\n",
            "Epoch 39/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0737\n",
            "Epoch 40/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0711\n",
            "Epoch 41/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0693\n",
            "Epoch 42/50\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 0.0685\n",
            "Epoch 43/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0698\n",
            "Epoch 44/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.0797\n",
            "Epoch 45/50\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.2247\n",
            "Epoch 46/50\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 0.0897\n",
            "Epoch 47/50\n",
            "200/200 [==============================] - ETA: 0s - loss: 0.0729Restoring model weights from the end of the best epoch: 42.\n",
            "200/200 [==============================] - 22s 108ms/step - loss: 0.0729\n",
            "Epoch 47: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, steps_per_epoch=STEPS_PER_EPOCH, callbacks=[checkpoint_callback, early_stopping_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvTbxfT9qL02"
      },
      "source": [
        "Generate text\n",
        "\n",
        "Restore the latest checkpoint\n",
        "To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size once built.\n",
        "\n",
        "To run the model with a different batch_size, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgI-Ekp6qO9T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f624584-bffd-48ff-d830-16c22e7adfce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (1, None, 256)            20736     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (1, None, 1024)           5246976   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (1, None, 1024)           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (1, None, 81)             83025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,350,737\n",
            "Trainable params: 5,350,737\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
          ]
        }
      ],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sydC-N4iqpfZ"
      },
      "source": [
        "The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Nwxb5OPEqtgS"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.3\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "8rYFxSWjrEC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2948d5c-42ef-4ccf-89c9-b7d77553beee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Great Gatsby of New York and then didn’t know—though\n",
            "\n",
            "her husband, was urged ar. Roby rrethe a rafed into swetthe laye; the grass on his lawn hid\n",
            "\n",
            "he got up and informed me, in an\n",
            "\n",
            "uncertain voice, that he was going home.\n",
            "\n",
            "\n",
            "\n",
            "“Why’s that?”\n",
            "\n",
            "\n",
            "\n",
            "“Nobody’s coming.”\n",
            "\n",
            "\n",
            "\n",
            "“Welve you beat the\n",
            "\n",
            "shoulder. At past one of the girls in yellowdy don’t\n",
            "\n",
            "you not a little afraid of missing something if I\n",
            "\n",
            "forget that, as my father snobbidythen watch\n",
            "\n",
            "answered the lethy was walking the words the\n",
            "\n",
            "whited—a nice right act and cashouse and excited young offincerst speads come and said that his shoulder\n",
            "\n",
            "in a dize or along the Sturn.\n",
            "\n",
            "\n",
            "\n",
            "“Your place looks like the World’s Fair,” I said.\n",
            "\n",
            "\n",
            "\n",
            "“Does it?” He turned his eyes toward it absently. “I have been\n",
            "\n",
            "glancing into some of the rooms. Let’s go so the window and, leaning forward, tapped on the\n",
            "\n",
            "frontom with a\n",
            "\n",
            "slight nod, and she winked at me again. “—And we’ve plut in the Sentincemar.”\n",
            "\n",
            "\n",
            "\n",
            "She looked at Tom alound. “And siTe dream, you. You\n",
            "\n",
            "absolute little dream.”\n",
            "\n",
            "\n",
            "\n",
            "“Yes,” admitted t\n"
          ]
        }
      ],
      "source": [
        "print(generate_text(model, start_string=\"The Great Gatsby of New York \"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "icp6_text_gen.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}